{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyQt6.QtCore\n",
    "import os\n",
    "os.environ[\"QT_API\"] = \"pyqt6\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use qt backend for matplotlab to use interactive mne plots\n",
    "%matplotlib qt\n",
    "\n",
    "import mne \n",
    "import analysis.processing\n",
    "import pandas as pd\n",
    "import csv \n",
    "import os\n",
    "from config import Config\n",
    "configObj = Config()\n",
    "from mne_connectivity import spectral_connectivity_time\n",
    "import numpy as np\n",
    "configss = configObj.getConfigSnapshot()\n",
    "from tqdm import tqdm\n",
    "import tools.helpers\n",
    "from scipy import stats\n",
    "\n",
    "mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 200 # 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 24/24 [00:00<00:00, 1135.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate PID-BlockType combinations after aggregation: 0\n",
      "\n",
      "Shapiro-Wilk p-values per BlockType:\n",
      "BlockType\n",
      "D     0.190438\n",
      "ND    0.000067\n",
      "Name: completion, dtype: float64\n",
      "\n",
      "Normality across all BlockTypes: True\n",
      "\n",
      "ANOVA Results:\n",
      "      Source          SS  DF          MS          F     p-unc       ng2  eps\n",
      "0  BlockType  255.616679   1  255.616679  14.688782  0.000852  0.139885  1.0\n",
      "1      Error  400.249902  23   17.402170        NaN       NaN       NaN  NaN\n",
      "\n",
      "Partial Eta Squared: 0.1399\n",
      "\n",
      "Descriptive Statistics and Effect Size:\n",
      "           Mean Completion  Std Deviation  Partial Eta Squared\n",
      "BlockType                                                     \n",
      "D                91.542969       6.417297             0.139885\n",
      "ND               96.158312       5.210947             0.139885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\erangad\\AppData\\Local\\Temp\\ipykernel_24528\\2225967126.py:109: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=('ci', 95)` for the same effect.\n",
      "\n",
      "  sns.barplot(x='BlockType', y='completion', data=aggregated_df, ci=95, palette='viridis')\n",
      "C:\\Users\\erangad\\AppData\\Local\\Temp\\ipykernel_24528\\2225967126.py:109: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='BlockType', y='completion', data=aggregated_df, ci=95, palette='viridis')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, wilcoxon\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# List of participant numbers excluding specified ones\n",
    "participant_numbers = [el for el in range(1, 32) if el not in [5, 13, 14, 16, 17, 20, 31]]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for pnum in tqdm(participant_numbers, desc=\"Loading Data\"):\n",
    "    participant_data_path = f\"{pnum}.csv\"\n",
    "    path_qa = os.path.join(configss['root'], configss['data_completion'], participant_data_path)\n",
    "    \n",
    "    # Read and append the DataFrame\n",
    "    df_participant = pd.read_csv(path_qa)\n",
    "    dataframes.append(df_participant)\n",
    "\n",
    "# Concatenate all DataFrames into one combined DataFrame\n",
    "combined_df = pd.concat(dataframes, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Function to replace outliers with median\n",
    "def replace_outliers(df, group_col, value_col):\n",
    "    df_copy = df.copy()\n",
    "    medians = df.groupby(group_col)[value_col].transform('median')\n",
    "    \n",
    "    Q1 = df[value_col].quantile(0.25)\n",
    "    Q3 = df[value_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace outliers with median\n",
    "    df_copy.loc[(df[value_col] < lower_bound) | (df[value_col] > upper_bound), value_col] = medians\n",
    "    return df_copy\n",
    "\n",
    "# Replace outliers in the combined data\n",
    "cleaned_df = replace_outliers(combined_df, 'BlockType', 'completion')\n",
    "\n",
    "# Aggregate the data\n",
    "aggregated_df = cleaned_df.groupby(['PID', 'BlockType'])['completion'].mean().reset_index()\n",
    "\n",
    "# Verify no duplicates\n",
    "duplicate_counts = aggregated_df.duplicated(subset=['PID', 'BlockType']).sum()\n",
    "print(f\"Number of duplicate PID-BlockType combinations after aggregation: {duplicate_counts}\")\n",
    "\n",
    "if duplicate_counts > 0:\n",
    "    raise ValueError(\"Duplicates found after aggregation. Please check your data.\")\n",
    "\n",
    "# Shapiro-Wilk normality test on the aggregated data\n",
    "shapiro_test = aggregated_df.groupby('BlockType')['completion'].apply(lambda x: shapiro(x)[1])\n",
    "print(f\"\\nShapiro-Wilk p-values per BlockType:\\n{shapiro_test}\")\n",
    "\n",
    "# Check normality\n",
    "normality = shapiro_test.all()  # True if all p-values > 0.05\n",
    "print(f\"\\nNormality across all BlockTypes: {normality}\\n\")\n",
    "\n",
    "if normality:\n",
    "    # Perform Repeated Measures ANOVA using pingouin\n",
    "    aov = pg.rm_anova(\n",
    "        data=aggregated_df,\n",
    "        dv='completion',\n",
    "        within='BlockType',\n",
    "        subject='PID',\n",
    "        detailed=True\n",
    "    )\n",
    "    print(\"ANOVA Results:\")\n",
    "    print(aov)\n",
    "    \n",
    "    # Extract partial eta squared\n",
    "    partial_eta_squared = aov.loc[aov['Source'] == 'BlockType', 'ng2'].values[0]\n",
    "    print(f\"\\nPartial Eta Squared: {partial_eta_squared:.4f}\")\n",
    "    \n",
    "    effect_size = partial_eta_squared\n",
    "else:\n",
    "    # Wilcoxon signed-rank test (assuming two BlockTypes)\n",
    "    block_types = aggregated_df['BlockType'].unique()\n",
    "    if len(block_types) != 2:\n",
    "        raise ValueError(\"Wilcoxon test requires exactly two BlockType conditions.\")\n",
    "    \n",
    "    group1, group2 = block_types\n",
    "    completion1 = aggregated_df[aggregated_df['BlockType'] == group1]['completion']\n",
    "    completion2 = aggregated_df[aggregated_df['BlockType'] == group2]['completion']\n",
    "    \n",
    "    wilcoxon_test_stat, wilcoxon_pvalue = wilcoxon(completion1, completion2)\n",
    "    \n",
    "    # Compute Z-statistic using normal approximation (due to ties)\n",
    "    N = len(completion1)\n",
    "    mean_rank = (N * (N + 1)) / 4\n",
    "    std_dev = np.sqrt((N * (N + 1) * (2 * N + 1)) / 24)\n",
    "    z_stat = (wilcoxon_test_stat - mean_rank) / std_dev\n",
    "    \n",
    "    # Effect size (r = Z / sqrt(N))\n",
    "    effect_size_corrected = abs(z_stat / np.sqrt(N))\n",
    "    \n",
    "    print(f\"\\nWilcoxon Signed-Rank Test Statistic: {wilcoxon_test_stat}\")\n",
    "    print(f\"Wilcoxon P-Value: {wilcoxon_pvalue:.4f}\")\n",
    "    print(f\"Effect Size (r): {effect_size_corrected:.4f}\")\n",
    "    \n",
    "    effect_size = effect_size_corrected\n",
    "\n",
    "# Plot the average completion for each BlockType with 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='BlockType', y='completion', data=aggregated_df, ci=95, palette='viridis')\n",
    "plt.title('Average Completion by Block Type')\n",
    "plt.xlabel('Block Type')\n",
    "plt.ylabel('Average Completion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "mean_completion = aggregated_df.groupby('BlockType')['completion'].mean()\n",
    "std_completion = aggregated_df.groupby('BlockType')['completion'].std()\n",
    "\n",
    "# Display the requested data\n",
    "if normality:\n",
    "    display_data = {\n",
    "        'Mean Completion': mean_completion,\n",
    "        'Std Deviation': std_completion,\n",
    "        'Partial Eta Squared': [effect_size] * len(mean_completion)\n",
    "    }\n",
    "else:\n",
    "    display_data = {\n",
    "        'Mean Completion': mean_completion,\n",
    "        'Std Deviation': std_completion,\n",
    "        'Wilcoxon Test Statistic': [wilcoxon_test_stat],\n",
    "        'Wilcoxon P-Value': [wilcoxon_pvalue],\n",
    "        'Effect Size (r)': [effect_size_corrected]\n",
    "    }\n",
    "\n",
    "display_df = pd.DataFrame(display_data)\n",
    "print(\"\\nDescriptive Statistics and Effect Size:\")\n",
    "print(display_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 24/24 [00:00<00:00, 904.46it/s]\n",
      "C:\\Users\\erangad\\AppData\\Local\\Temp\\ipykernel_24528\\2087868017.py:220: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=('ci', 95)` for the same effect.\n",
      "\n",
      "  sns.barplot(x='BlockType', y='completion', data=aggregated_df, ci=95, palette='viridis')\n",
      "C:\\Users\\erangad\\AppData\\Local\\Temp\\ipykernel_24528\\2087868017.py:220: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='BlockType', y='completion', data=aggregated_df, ci=95, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing 16 outliers in 'completion' within each 'BlockType' group.\n",
      "\n",
      "Number of duplicate PID-BlockType combinations after aggregation: 0\n",
      "\n",
      "Number of BlockTypes: 2\n",
      "Block Types identified for paired comparison: 'D' and 'ND'\n",
      "\n",
      "Shapiro-Wilk Test for Differences between 'D' and 'ND':\n",
      "Statistic = 0.9387, p-value = 0.1526\n",
      "\n",
      "Normality of Differences: Pass\n",
      "\n",
      "Data is normally distributed. Performing Paired t-test.\n",
      "\n",
      "Paired t-test Results:\n",
      "t-statistic = -3.8326, p-value = 0.0009\n",
      "Cohen's d = -0.7823\n",
      "\n",
      "Descriptive Statistics and Effect Size:\n",
      "           Mean Completion  Std Deviation  Effect Size (Cohen's d)\n",
      "BlockType                                                         \n",
      "D                91.542969       6.417297                -0.782325\n",
      "ND               96.158312       5.210947                -0.782325\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, wilcoxon, ttest_rel\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Data Loading and Cleaning\n",
    "# -----------------------------\n",
    "\n",
    "# List of participant numbers excluding specified ones\n",
    "participant_numbers = [el for el in range(1, 32) if el not in [5, 13, 14, 16, 17, 20, 31]]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for pnum in tqdm(participant_numbers, desc=\"Loading Data\"):\n",
    "    participant_data_path = f\"{pnum}.csv\"\n",
    "    path_qa = os.path.join(configss['root'], configss['data_completion'], participant_data_path)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(path_qa):\n",
    "        print(f\"Warning: File {path_qa} does not exist. Skipping participant {pnum}.\")\n",
    "        continue\n",
    "    \n",
    "    # Read and append the DataFrame\n",
    "    try:\n",
    "        df_participant = pd.read_csv(path_qa)\n",
    "        df_participant['PID'] = pnum  # Ensure 'PID' column exists\n",
    "        dataframes.append(df_participant)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path_qa}: {e}\")\n",
    "\n",
    "# Check if any data was loaded\n",
    "if not dataframes:\n",
    "    raise ValueError(\"No participant data loaded. Please check file paths and data.\")\n",
    "\n",
    "# Concatenate all DataFrames into one combined DataFrame\n",
    "combined_df = pd.concat(dataframes, axis=0).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Outlier Replacement\n",
    "# -----------------------------\n",
    "\n",
    "def replace_outliers(df, group_col, value_col):\n",
    "    \"\"\"\n",
    "    Replace outliers in 'value_col' within each 'group_col' group with the group's median.\n",
    "    Outliers are defined as values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    medians = df.groupby(group_col)[value_col].transform('median')\n",
    "    \n",
    "    Q1 = df[value_col].quantile(0.25)\n",
    "    Q3 = df[value_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace outliers with median\n",
    "    outliers = (df[value_col] < lower_bound) | (df[value_col] > upper_bound)\n",
    "    num_outliers = outliers.sum()\n",
    "    print(f\"Replacing {num_outliers} outliers in '{value_col}' within each '{group_col}' group.\")\n",
    "    df_copy.loc[outliers, value_col] = medians[outliers]\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Replace outliers in the combined data\n",
    "cleaned_df = replace_outliers(combined_df, 'BlockType', 'completion')\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Data Aggregation\n",
    "# -----------------------------\n",
    "\n",
    "# Aggregate the data by computing mean 'completion' per PID and BlockType\n",
    "aggregated_df = cleaned_df.groupby(['PID', 'BlockType'])['completion'].mean().reset_index()\n",
    "\n",
    "# Verify no duplicates\n",
    "duplicate_counts = aggregated_df.duplicated(subset=['PID', 'BlockType']).sum()\n",
    "print(f\"\\nNumber of duplicate PID-BlockType combinations after aggregation: {duplicate_counts}\")\n",
    "\n",
    "if duplicate_counts > 0:\n",
    "    raise ValueError(\"Duplicates found after aggregation. Please check your data.\")\n",
    "\n",
    "# Ensure that each PID has data for all BlockTypes\n",
    "block_types = aggregated_df['BlockType'].unique()\n",
    "block_type_counts = aggregated_df.groupby('PID')['BlockType'].nunique()\n",
    "\n",
    "missing_blocktypes = block_type_counts[block_type_counts != len(block_types)].index.tolist()\n",
    "if missing_blocktypes:\n",
    "    print(f\"\\nParticipants missing some BlockTypes: {missing_blocktypes}\")\n",
    "    # Exclude these participants from the analysis\n",
    "    aggregated_df = aggregated_df[~aggregated_df['PID'].isin(missing_blocktypes)]\n",
    "    print(f\"Excluded participants missing BlockTypes. Remaining participants: {aggregated_df['PID'].nunique()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Normality Testing on Differences\n",
    "# -----------------------------\n",
    "\n",
    "# Pivot the data to have one row per participant with columns for each BlockType\n",
    "pivot_df = aggregated_df.pivot(index='PID', columns='BlockType', values='completion')\n",
    "\n",
    "# Check the number of BlockTypes\n",
    "num_blocktypes = len(block_types)\n",
    "print(f\"\\nNumber of BlockTypes: {num_blocktypes}\")\n",
    "\n",
    "if num_blocktypes == 2:\n",
    "    # Exactly two BlockTypes: proceed with paired tests\n",
    "    block1, block2 = block_types\n",
    "    print(f\"Block Types identified for paired comparison: '{block1}' and '{block2}'\")\n",
    "    \n",
    "    # Calculate differences\n",
    "    pivot_df['Difference'] = pivot_df[block1] - pivot_df[block2]\n",
    "    \n",
    "    # Shapiro-Wilk test on differences\n",
    "    shapiro_stat, shapiro_p = shapiro(pivot_df['Difference'])\n",
    "    print(f\"\\nShapiro-Wilk Test for Differences between '{block1}' and '{block2}':\")\n",
    "    print(f\"Statistic = {shapiro_stat:.4f}, p-value = {shapiro_p:.4f}\")\n",
    "    \n",
    "    # Check normality based on differences\n",
    "    normality = shapiro_p > 0.05\n",
    "    print(f\"\\nNormality of Differences: {'Pass' if normality else 'Fail'}\")\n",
    "else:\n",
    "    # More than two BlockTypes: consider Repeated Measures ANOVA or Friedman Test\n",
    "    print(\"\\nMore than two BlockTypes detected. Repeated Measures ANOVA will be performed assuming normality.\")\n",
    "    # Proceed with Repeated Measures ANOVA without normality assumption\n",
    "    # Alternatively, implement a Friedman Test for non-parametric analysis\n",
    "    # For simplicity, we'll proceed with ANOVA but note the assumption\n",
    "    differences = pivot_df[block_types].diff(axis=1).iloc[:, -1].dropna()\n",
    "    shapiro_stat, shapiro_p = shapiro(differences)\n",
    "    print(f\"\\nShapiro-Wilk Test for Differences among BlockTypes: Statistic = {shapiro_stat:.4f}, p-value = {shapiro_p:.4f}\")\n",
    "    \n",
    "    normality = shapiro_p > 0.05\n",
    "    print(f\"\\nNormality of Differences: {'Pass' if normality else 'Fail'}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Statistical Testing\n",
    "# -----------------------------\n",
    "\n",
    "if num_blocktypes == 2:\n",
    "    if normality:\n",
    "        print(\"\\nData is normally distributed. Performing Paired t-test.\")\n",
    "        # Perform Paired t-test\n",
    "        t_stat, t_p_value = ttest_rel(pivot_df[block1], pivot_df[block2])\n",
    "        print(f\"\\nPaired t-test Results:\")\n",
    "        print(f\"t-statistic = {t_stat:.4f}, p-value = {t_p_value:.4f}\")\n",
    "        \n",
    "        # Calculate Cohen's d for paired samples\n",
    "        differences = pivot_df['Difference']\n",
    "        cohen_d = differences.mean() / differences.std(ddof=1)\n",
    "        print(f\"Cohen's d = {cohen_d:.4f}\")\n",
    "        \n",
    "        # Effect size as per t-test\n",
    "        effect_size = cohen_d\n",
    "    else:\n",
    "        print(\"\\nData is not normally distributed. Performing Wilcoxon signed-rank test.\")\n",
    "        # Perform Wilcoxon signed-rank test\n",
    "        # Remove zero differences as Wilcoxon cannot handle them\n",
    "        non_zero = pivot_df['Difference'] != 0\n",
    "        completion1 = pivot_df.loc[non_zero, block1]\n",
    "        completion2 = pivot_df.loc[non_zero, block2]\n",
    "        \n",
    "        wilcoxon_test_stat, wilcoxon_pvalue = wilcoxon(completion1, completion2, alternative='two-sided')\n",
    "        print(f\"\\nWilcoxon Signed-Rank Test Results:\")\n",
    "        print(f\"Statistic = {wilcoxon_test_stat}, p-value = {wilcoxon_pvalue:.4f}\")\n",
    "        \n",
    "        # Calculate Effect Size (Rank Biserial Correlation) using pingouin\n",
    "        # Since Scipy does not provide Z-statistic directly, we use pingouin for effect size\n",
    "        effect_size = pg.compute_effsize(completion1, completion2, paired=True, eftype='r')\n",
    "        print(f\"Effect Size (Rank Biserial Correlation) = {effect_size:.4f}\")\n",
    "else:\n",
    "    if normality:\n",
    "        print(\"\\nData is normally distributed. Performing Repeated Measures ANOVA.\")\n",
    "        # Perform Repeated Measures ANOVA using pingouin\n",
    "        aov = pg.rm_anova(\n",
    "            data=aggregated_df,\n",
    "            dv='completion',\n",
    "            within='BlockType',\n",
    "            subject='PID',\n",
    "            detailed=True\n",
    "        )\n",
    "        print(\"\\nANOVA Results:\")\n",
    "        print(aov)\n",
    "        \n",
    "        # Extract partial eta squared\n",
    "        partial_eta_squared = aov.loc[aov['Source'] == 'BlockType', 'np2'].values[0]\n",
    "        print(f\"\\nPartial Eta Squared: {partial_eta_squared:.4f}\")\n",
    "        \n",
    "        effect_size = partial_eta_squared\n",
    "    else:\n",
    "        print(\"\\nData is not normally distributed. Performing Friedman test.\")\n",
    "        from scipy.stats import friedmanchisquare\n",
    "        \n",
    "        # Prepare data for Friedman test\n",
    "        friedman_data = [pivot_df[bt].dropna() for bt in block_types]\n",
    "        \n",
    "        # Perform Friedman test\n",
    "        friedman_stat, friedman_pvalue = friedmanchisquare(*friedman_data)\n",
    "        print(f\"\\nFriedman Test Results:\")\n",
    "        print(f\"Statistic = {friedman_stat:.4f}, p-value = {friedman_pvalue:.4f}\")\n",
    "        \n",
    "        # Calculate Effect Size (Partial Eta Squared)\n",
    "        # Formula: η² = (chi²) / (N * (k - 1))\n",
    "        N = pivot_df.shape[0]\n",
    "        k = num_blocktypes\n",
    "        partial_eta_squared = friedman_stat / (N * (k - 1))\n",
    "        print(f\"Partial Eta Squared = {partial_eta_squared:.4f}\")\n",
    "        \n",
    "        effect_size = partial_eta_squared\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Visualization\n",
    "# -----------------------------\n",
    "\n",
    "# Plot the average completion for each BlockType with 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='BlockType', y='completion', data=aggregated_df, ci=95, palette='viridis')\n",
    "plt.title('Average Completion by Block Type')\n",
    "plt.xlabel('Block Type')\n",
    "plt.ylabel('Average Completion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Descriptive Statistics and Effect Size Reporting\n",
    "# -----------------------------\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "mean_completion = aggregated_df.groupby('BlockType')['completion'].mean()\n",
    "std_completion = aggregated_df.groupby('BlockType')['completion'].std()\n",
    "\n",
    "# Display the requested data\n",
    "if num_blocktypes == 2:\n",
    "    if normality:\n",
    "        display_data = {\n",
    "            'Mean Completion': mean_completion,\n",
    "            'Std Deviation': std_completion,\n",
    "            'Effect Size (Cohen\\'s d)': [effect_size] * len(mean_completion)\n",
    "        }\n",
    "    else:\n",
    "        display_data = {\n",
    "            'Mean Completion': mean_completion,\n",
    "            'Std Deviation': std_completion,\n",
    "            'Wilcoxon Test Statistic': [wilcoxon_test_stat],\n",
    "            'Wilcoxon P-Value': [wilcoxon_pvalue],\n",
    "            'Effect Size (r)': [effect_size]\n",
    "        }\n",
    "else:\n",
    "    if normality:\n",
    "        display_data = {\n",
    "            'Mean Completion': mean_completion,\n",
    "            'Std Deviation': std_completion,\n",
    "            'Partial Eta Squared': [effect_size] * len(mean_completion)\n",
    "        }\n",
    "    else:\n",
    "        display_data = {\n",
    "            'Mean Completion': mean_completion,\n",
    "            'Std Deviation': std_completion,\n",
    "            'Friedman Test Statistic': [friedman_stat],\n",
    "            'Friedman P-Value': [friedman_pvalue],\n",
    "            'Effect Size (Partial Eta Squared)': [effect_size]\n",
    "        }\n",
    "\n",
    "display_df = pd.DataFrame(display_data)\n",
    "print(\"\\nDescriptive Statistics and Effect Size:\")\n",
    "print(display_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>BlockType</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ND</td>\n",
       "      <td>96.614583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>88.307292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ND</td>\n",
       "      <td>98.151042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>93.411458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>D</td>\n",
       "      <td>90.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>ND</td>\n",
       "      <td>99.401042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>D</td>\n",
       "      <td>91.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>ND</td>\n",
       "      <td>90.325521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>D</td>\n",
       "      <td>85.963542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>D</td>\n",
       "      <td>86.432292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>D</td>\n",
       "      <td>83.567708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>ND</td>\n",
       "      <td>91.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>D</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>D</td>\n",
       "      <td>85.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11</td>\n",
       "      <td>ND</td>\n",
       "      <td>82.760417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>D</td>\n",
       "      <td>78.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>ND</td>\n",
       "      <td>90.364583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>D</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18</td>\n",
       "      <td>D</td>\n",
       "      <td>94.244792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18</td>\n",
       "      <td>ND</td>\n",
       "      <td>94.557292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19</td>\n",
       "      <td>D</td>\n",
       "      <td>88.411458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19</td>\n",
       "      <td>ND</td>\n",
       "      <td>98.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>21</td>\n",
       "      <td>D</td>\n",
       "      <td>87.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21</td>\n",
       "      <td>ND</td>\n",
       "      <td>92.630208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>22</td>\n",
       "      <td>D</td>\n",
       "      <td>81.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22</td>\n",
       "      <td>ND</td>\n",
       "      <td>93.411458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>23</td>\n",
       "      <td>D</td>\n",
       "      <td>91.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>23</td>\n",
       "      <td>ND</td>\n",
       "      <td>83.489583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>24</td>\n",
       "      <td>D</td>\n",
       "      <td>99.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24</td>\n",
       "      <td>ND</td>\n",
       "      <td>98.723958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25</td>\n",
       "      <td>D</td>\n",
       "      <td>99.739583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>26</td>\n",
       "      <td>D</td>\n",
       "      <td>91.614583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>26</td>\n",
       "      <td>ND</td>\n",
       "      <td>99.036458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>27</td>\n",
       "      <td>D</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>27</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>28</td>\n",
       "      <td>D</td>\n",
       "      <td>89.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>28</td>\n",
       "      <td>ND</td>\n",
       "      <td>98.255208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>29</td>\n",
       "      <td>D</td>\n",
       "      <td>96.744792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>29</td>\n",
       "      <td>ND</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>30</td>\n",
       "      <td>D</td>\n",
       "      <td>99.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>30</td>\n",
       "      <td>ND</td>\n",
       "      <td>99.765625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PID BlockType  completion\n",
       "0     1         D   95.000000\n",
       "1     1        ND   96.614583\n",
       "2     2         D   88.307292\n",
       "3     2        ND   98.151042\n",
       "4     3         D   93.411458\n",
       "5     3        ND  100.000000\n",
       "6     4         D   90.833333\n",
       "7     4        ND   99.401042\n",
       "8     6         D   91.458333\n",
       "9     6        ND   90.325521\n",
       "10    7         D   85.963542\n",
       "11    7        ND  100.000000\n",
       "12    8         D   86.432292\n",
       "13    8        ND  100.000000\n",
       "14    9         D   83.567708\n",
       "15    9        ND   91.458333\n",
       "16   10         D  100.000000\n",
       "17   10        ND  100.000000\n",
       "18   11         D   85.078125\n",
       "19   11        ND   82.760417\n",
       "20   12         D   78.281250\n",
       "21   12        ND   90.364583\n",
       "22   15         D  100.000000\n",
       "23   15        ND  100.000000\n",
       "24   18         D   94.244792\n",
       "25   18        ND   94.557292\n",
       "26   19         D   88.411458\n",
       "27   19        ND   98.854167\n",
       "28   21         D   87.083333\n",
       "29   21        ND   92.630208\n",
       "30   22         D   81.354167\n",
       "31   22        ND   93.411458\n",
       "32   23         D   91.458333\n",
       "33   23        ND   83.489583\n",
       "34   24         D   99.296875\n",
       "35   24        ND   98.723958\n",
       "36   25         D   99.739583\n",
       "37   25        ND  100.000000\n",
       "38   26         D   91.614583\n",
       "39   26        ND   99.036458\n",
       "40   27         D  100.000000\n",
       "41   27        ND  100.000000\n",
       "42   28         D   89.062500\n",
       "43   28        ND   98.255208\n",
       "44   29         D   96.744792\n",
       "45   29        ND  100.000000\n",
       "46   30         D   99.687500\n",
       "47   30        ND   99.765625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
